
\documentclass[a4paper]{scrartcl}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{chngpage}


\KOMAoption{captions}{bottombeside}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\author{
  Winter, Felix\\
  \texttt{e0825516@student.tuwien.ac.at}
  \and
  Wagner, Christian\\
  \texttt{e0725942@student.tuwien.ac.at}
}
\title{Programming Exercise Heuristic Optimization Techniques 2014/2015}


\begin{document}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}

\begingroup
 \makeatletter
 %\@titlepagetrue
 \maketitle
\endgroup

\section{Description of the algorithms}

\subsection{Greedy Construction Heuristic}

We use a very simple construction heuristic: For each node we create a list of potential neighbours sorted by the arc costs.
This is done in such a way that for the zero node and demand nodes weights to all supply nodes are stored. In the case of the supply
nodes all demand nodes are potential neighbours.
To create the tours always the cheapest direct path to a neighbour is chosen. Nodes that have been visited will be removed from that
pre-sorted list. The heuristic tries to make a tour as long as possible, therefore a new tour is started only if the time limit would be exceeded.
If there are no more vehicles available, the last tour might become infeasible. For this reason the heuristic does not necessarily
produce a feasible solution.
\subsection{Very Large Neighbourhood Search}

Our algorithm uses a combination of Constraint Programming and Local Search methods. We basically do a Very Large Neighbourhood Search, which uses a Tree Search to find good neighbours of a given solution.
A lot of our implementation ideas were based on the work of Paul Shaw \cite{shaw1998using}. We applied the procedure described in his paper and modified it to fit our problem description.

At each iteration of local search we remove randomly chosen pairs of nodes and try to reinsert them into valid slots. So for example a pair $S \rightarrow D$ can be inserted between a zero and a supply node, whereas a $D \rightarrow S$ pair can only be inserted between a supply and a demand node. By this procedure it is also possible to create new tours if there are vehicles left or to destroy a tour if all its nodes are removed. Since we do a whole Tree Search over all possible insertion slots for a multitude of pairs, finding the best neighbour can be a time consuming task. To reduce the workload we try to make the tree as small as possible with a branch and bound technique similar to the one described in \cite{shaw1998using}
As an upper bound the cost of the so far best found solution is taken. To create a lower bound we use the following formula:

$lowerBound = 3 \cdot minArcCost \cdot numPairs - maxArcCost \cdot numPairs$

We also use a simple variable selection heuristic for our Tree Search: Pairs that are connected with a higher cost are chosen first. This is done because for one thing violations of the time limit might be reached earlier in the tree and for another thing later in the tree this costly arcs might be destroyed by other re-insertions and therefore affect the costs positively.

We iteratively increase the number of removed pairs until a better neighbour is found. The way the pairs are chosen is done randomly although we try to prefer pairs that are connected with low costs with a certain factor. As soon as the algorithm cannot find any improvements in a number of iterations the program will output the best solution it has found.

The size of the search tree is mainly dependent on the number of node pairs that are removed and reinserted and therefore has a complexity of $O(n^{maxRemoves})$.

\subsection{Greedy Randomized Adaptive Search Procedure}
We iteratively generate solutions using a randomized construction heuristic and apply our Very Large Neighbourhood Search on all generated solutions. In the end we take the overall best solution found.

For the randomized greedy construction heuristic we modified our greedy heuristic as described in the lecture. We build a Restricted Candidate List from our neighbournode candidates with a use of a parameter $alpha$ and choose the candidates by a certain random chance.

\subsection{Description of the parameters}
Our algorithm is configured by a number of parameters that affect the search:

\begin{itemize}
  \item startRemoves: How many node pairs should be removed during neighbourhood search for a start. The number of removes increases during the overall search process.
  \item removeLimit: The upper limit of removes that should be performed during neighbourhood search. If the maximum is reached and search cannot find any better solution the overall local search ends.

  \item trials: The number of trials that are performed with each number of pair removes until the removecount will be increased. If trials is for example 5 the search has to perform 5 iterations without finding any better solution before the number of pair removes is increased by one.

  \item relatedness: This integer value has an effect on the choice of pairs that are selected for removal during neighbourhood search. If relatedness is $\infty$ the pairs will be ranked totally depending on their connecting arc costs. With a relatedness of 1 the choice will be made completely at random. Any value in between makes a tradeoff.

  \item greediness: This real value between 0 and 1 influences the generation of the Restricted Candidate List generated during the Randomized Greedy Construction heuristic. A value of 0 will use a totally greedy heuristic, whereas a value of 1 will only make random node selections to build the initial solution. Any value in between makes a tradeoff.
\end{itemize}

\section{Parameter Tuning}
We did some manual testing with parameters on the instances at first. The number of removes has a high influence on the running time and we therefore set the removeLimit on lower values for instances with a large number of nodes. Regarding the trials parameters 15 seemed to be a decent value for trials and 5 for graspTrials.

To find out good values for relatedness and greediness we used irace \cite{lopez2011irace} with mostly default parameters. We only limited the maximal number of allowed runs for irace to 300 because time was running short. irace determined that a relatedness of about 2 and a greediness of 0.12 are good parameter choices.

\section{Results and Discussion}

We performed experimental runs on instances with a different number of nodes. The results are shown in Table 1. For each run we performed 30 runs with the following parameters:

\begin{itemize}
 \item startRemoves 2
 \item removeLimit 4
 \item relatedness 2
 \item greediness 0.12
 \item trials 15
 \item graspTrials 5
\end{itemize}

Note that runs that are marked with a star (*) use a removeLimit of 3 and runs marked with two stars (**) use a removeLimit of 2 instead.

Each experiment was run 30 times, and then results of the best run were kept.

One can clearly see that the greedy heuristic alone is easily outperformed by the approaches that use VLNS and GRASP.  With the use of GRASP a small improvement over VLNS can be achieved, however at the cost of runtime. Unfortunately we had to reduce the removeLimits for larger instances a bit, to be able to conduct all the necessary runs in time. Executing GRASP with a removeLimit of 4 would probably lead to even better results.

\begin{table}
    \begin{adjustwidth}{-.5in}{-.5in}  
        \begin{center}
\begin{tabular}{r | r | r | r | r | r | r | r | r}
\hline
Nodes & Gr.Heu. & Time &  VLNS & Time & Total & GRASP & Time & Total \\
\hline \hline 
10 & 356 & 0 & 347 & 0.0037 & 0.1663 & 347 & 0.0335 & 0.8766 \\
\hline
30 & 798 & 0.0001 & 767 & 0.1394 & 3.0000 & 764 & 1.4821 & 26.4703 \\
\hline
60 & 1841 & 0.0007 & 1712 & 2.8097 & 16.3781 & 1711 & 9.4027 & 315.7900 \\
\hline
90 & 2171 & 0.0017 & 2055 & 13.6852 & 222.8823 & 2055 & 67.0838 & 1709.3321\\
\hline
120 & 2844 & 0.0050 & 2745 & 17.5720 & 194.1892 & *2712 & 6.6881 & 200.7002 \\
\hline
180 & 4874 & 0.0071 & *4651 & 7.8030 & 106.1305 & *4623 & 36.1978 & 1216.7366 \\
\hline
300 & 9390 & 0.0210 & *8833 & 7.9615 & 138.3080 & *8782 & 48.0761 & 1457.6698 \\
\hline
400 & 11374 & 0.0387 & *10929 & 32.6781 & 482.6105 & **11073 & 21.5001 & 1061.1469 \\
\hline
500 & 13922 & 0.0627 & **13514 & 7.5111 & 97.4571 & **13463 & 71.5584 & 2753.0495 \\
\hline
700 & 19007 & 0.1300 & **18354 & 21.4090 & 396.1424 & **18311 & 447.6060 & 9254.5938 \\
\hline
\end{tabular}

        \caption{Table with best results from the runs. }
        \label{myTable}
        \end{center}
    \end{adjustwidth}
\end{table}

\bibliographystyle{plain}
\bibliography{literature0}

\end{document}


